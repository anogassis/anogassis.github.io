---
layout: post
title: AGI Safety Fundamentals - Week 0
date: 2024-02-21
categories: [AGI Safety Fundamentals]
---

# Week 0 Lecture Notes

## Intro to Large Language Models

- Llama 2 70B used 10TB of text, 6000 GPUs for 12 days, costing ~$2M and using ~1e24 FLOPS.
- Training runs of SOTA models runs in tens to hundreds of millions of dollars.
- Neural Nets are empirical artifacts that require sofisticated evaluations to work with these models.
- Pre-training is about acquiring knowledge. Fine-tunning is about alignment.

## Readings

- [ ] [[1hr Talk] Intro to Large Language Models](https://www.youtube.com/watch?v=zjkBMFhNj_g)
- [ ] [Title of Reading 2](link-to-reading-2)
- [ ] [Title of Reading 3](link-to-reading-3)

## Exercises

- [ ] Exercise 1: Description of Exercise 1
- [ ] Exercise 2: Description of Exercise 2
- [ ] Exercise 3: Description of Exercise 3

## Additional Notes
